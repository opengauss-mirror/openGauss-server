# ----------------------
# MOT configuration file
# ----------------------
#
# This file consists of lines of the form:
#
#   name = value
#
# Whitespace may be used.  Comments are introduced with "#" anywhere on a line.
#
# The commented-out settings shown in this file represent the default values.
# Re-commenting a setting is NOT sufficient to revert it to the default value;
# you need to reload the server.
#
# This file is read ONLY on server startup. If you edit the file on a running system,
# you have to reload the server for the changes to take effect.
#
# Memory units:  KB = kilobytes
#                MB = megabytes
#                GB = gigabytes
#                TB = terabytes
#
# If no memory units are specified, then kilobytes are assumed.
#
# Some memory units can be also given in the form of percentage from max_process_memory configured
# in postgresql.conf as follows:
#
#   20%
#
# Time units:  ms  = milliseconds (can also specify millis or milliseconds)
#              s   = seconds (can also specify secs or seconds)
#              min = minutes (can also specify mins or minutes)
#              h   = hours (can also specify hours)
#              d   = days (can also specify days)
#
# If no time units are specified, then milliseconds are assumed.

#------------------------------------------------------------------------------
# REDO LOG
#------------------------------------------------------------------------------

# Specifies whether to use group commit.
# This option is relevant only when openGauss is configured to use synchronous commit (i.e. when
# postgresql.conf has synchronous_commit configured to any value other than 'off').
#
#enable_group_commit = false

# Defines the group size and timeout when group commit is enabled.
# Each commit group is closed when either the configured number of transactions arrive or when the
# configured timeout passes since the group was opened. When the group is closed, all of the
# transactions in the group wait for a group-flush to finish executing, before notifying the client
# that each transaction ended.
#
#group_commit_size = 16
#group_commit_timeout = 10 ms

# Specifies the number of redo-log buffers to use for asynchronous commit mode.
# Allowed range of values for this configuration is [8, 128]. The size of one buffer is 128 MB.
# This option is relevant only when openGauss is configured to use asynchronous commit (i.e. when
# postgresql.conf has synchronous_commit configured to 'off').
#
#async_log_buffer_count = 24

#------------------------------------------------------------------------------
# CHECKPOINT
#------------------------------------------------------------------------------

# Specifies the directory in which checkpoint data is to be stored.
# The default location is in the data folder of each data node.
#
#checkpoint_dir = 

# Specifies the segment size used during checkpoint.
# Checkpoint is performed in segments. When a segments is full, it is serialized into disk and a
# new segment is opened for the subsequent checkpoint data.
# Note: Percentage values cannot be set for this configuration item.
#
#checkpoint_segsize = 16 MB

# Specifies the number of workers to use during checkpoint.
# Checkpoint is performed in parallel by several workers. The number of workers may affect
# substantially the overall performance of the entire checkpoint operation as well as the operation
# of other running transactions. In order to achieve shorter checkpoint duration, a higher number
# of workers should be used, up to the optimal number (which varies according to hardware and
# workload), at the expense of affecting the execution time of other running transactions. In order
# to ensure minimal effect on runtime of other running transactions, but at the cost of longer
# checkpoint durations, keep this number low.
#
#checkpoint_workers = 3

#------------------------------------------------------------------------------
# RECOVERY
#------------------------------------------------------------------------------

# Specifies the number of workers to use during checkpoint data recovery.
#
#checkpoint_recovery_workers = 3

# Specifies the number of workers to use during redo recovery/replay.
#
#parallel_recovery_workers = 5

# Specifies the size of the queue used during recovery to hold redo log segments.
# This parameter also limits the maximum number of transactions that can be active (in progress)
# during parallel recovery. If this limit is reached, redo replay will wait for some of the
# transactions to commit before processing the redo log for new transactions.
#
#parallel_recovery_queue_size = 512

#------------------------------------------------------------------------------
# STATISTICS
#------------------------------------------------------------------------------

# Configures periodic statistics printing.
#
#enable_stats = false

# Configures the time period for printing a summary statistics report.
#
#print_stats_period = 10 minutes

# Configures the time period for printing a full statistics report.
#
#print_full_stats_period = 1 hours

# Configures the sections included in the periodic statistics reports. If none is configured then
# the statistics report is suppressed.
# The log recovery statistics contains various redo-log recovery metrics.
# The DB session statistics contains transaction events such commits, rollback etc.
# The network statistics contains connection/disconnection events.
# The log statistics contains details regarding redo-log.
# The memory statistics contains memory layer details.
# The process statistics contains total memory and CPU consumption for the current process.
# The system statistics contains total memory and CPU consumption for the entire system.
# The JIT statistics contains regarding JIT query compilation and execution.
#
#enable_log_recovery_stats = false
#enable_db_session_stats = false
#enable_network_stats = false
#enable_log_stats = false
#enable_memory_stats = false
#enable_process_stats = false
#enable_system_stats = false
#enable_jit_stats = false

#------------------------------------------------------------------------------
# ERROR LOG
#------------------------------------------------------------------------------

# Configures the log level of messages issued by MOT engine and recorded in the error log of the
# database server.
# Valid values are: PANIC, ERROR, WARN, INFO, TRACE, DEBUG, DIAG1, DIAG2
#
#log_level = INFO

# Configure specific loggers according to the following syntax:
#
#   Log.COMPONENT.LOGGER.log_level=LOG_LEVEL
#
# For instance, configure TRACE log level for the ThreadIdPool logger in System component as follows:
#
#   Log.System.ThreadIdPool.log_level=TRACE
#
# In order the configure log level for all loggers under some component, use the following syntax:
#
#   Log.COMPONENT.log_level=LOG_LEVEL
#
# For instance:
#
#   Log.System.log_level=DEBUG

#------------------------------------------------------------------------------
# MEMORY
#------------------------------------------------------------------------------

# Specifies whether to use NUMA-aware memory allocation.
# When disabled, all affinity configurations are disabled as well.
# MOT engine assumes that all the available NUMA nodes have memory. If the machine has some special
# configuration in which some of the NUMA nodes have no memory, then this configuraton will be
# disabled internally and the MOT engine will function without using NUMA-aware memory allocation.
#
#enable_numa = true

# Configures the affinity mode of threads for user session and internal MOT tasks.
# When a thread-pool is used this value is ignored for user sessions (since their affinity is
# governed by the thread-pool), but still used for internal MOT tasks.
# Valid values are: fill-socket-first, equal-per-socket, fill-physical-first, none.
# fill-socket-first attaches threads to cores in the same socket until the socket is full and then
# moves on to the next socket.
# equal-per-socket spreads threads evenly among all sockets.
# fill-physical-first attaches threads to physical cores in the same socket until all physical
# cores are employed and then moves to the next socket. When all physical cores are used, then the
# process begins again with hyper-threaded cores.
# None disables any affinity configuration and lets the system scheduler decide on which core each
# thread is scheduled to run.
#
#affinity_mode = equal-per-socket

# Configures the chunk directory mode (used for memory chunk lookup).
# Lazy mode configures the chunk directory to load parts of it on demand and therefore reduces the
# initial memory footprint (from 1 GB to 1 MB approximately) but could result in minor performance
# penalties, and errors in extreme conditions of memory distress. On the other hand, using a non-
# lazy chunk directory does result in additional 1 GB of initial memory allocation, but results in
# slightly higher performance, and ensures chunk directory errors are avoided during memory distress.
#
#lazy_load_chunk_directory = true

# Configures the memory reservation mode (either physical or virtual).
# Whenever memory is allocated from kernel, this configuration value is consulted to determine
# whether the allocated memory is to be resident (physical) or not (virtual). This relates mostly
# to pre-allocation, but also to runtime allocations. In case of a physical reservation mode, the
# entire allocated memory region is made resident by forcing page faults on all pages spanned by
# the memory region. Configuring virtual memory reservation could result in faster memory
# allocation (especially during pre-allocation phase), but may result in page-faults during first
# access (and thus may result in slight performance hit), and more sever errors in case physical
# memory is unavailable. On the other hand, physical memory allocation is slower, but later access
# is both faster and guaranteed.
#
#reserve_memory_mode = virtual

# Configures the memory storage policy (compact or expanding).
# When compact policy is defined, unused memory is released back to the kernel, until the lower
# memory limit is reached (see min_mot_memory below). In expanding policy, unused memory is stored
# in the MOT engine for later reuse. A compact storage policy would reduce the memory footprint of
# the MOT engine, but might result occasionally in minor performance degradation. In addition it
# might result in unavailable memory during memory distress. On the other hand, expanding mode uses
# more memory, but allows both for faster memory allocation and has a greater guarantee that memory
# can be re-allocated after being de-allocated.
#
#store_memory_policy = compact

# Configures the chunk allocation policy for global memory.
# Available values: auto, local, page-interleaved, chunk-interleaved, native.
# MOT memory is organized in chunks of 2 megabyte each. The source NUMA node and the memory layout
# of each chunk affects the spread of table data among NUMA nodes, and therefore can significantly
# affect the data access time. Whenever allocating a chunk on a specific NUMA node the allocation
# policy is consulted.
# Auto policy selects a chunk allocation policy based on the current hardware.
# Local policy allocates each chunk on its respective NUMA node.
# Page-interleaved policy allocates chunks that are composed of interleaved memory 4 kilobyte pages
# from all NUMA nodes.
# Chunk-interleaved policy allocates chunks in a round robin fashion from all NUMA nodes.
# Native policy allocates chunks by calling the native system memory allocator.
#
#chunk_alloc_policy = auto

# Configures the number of worker per NUMA node participating in memory pre-allocation.
#
#chunk_prealloc_worker_count = 8

# Configures the maximum memory limit for the global memory of the MOT engine.
# Specifying percentage value relates to the total defined by max_process_memory configured in
# postgresql.conf.
# MOT engine memory is divided into global (long-term) memory that is mainly used to store user
# data, and local (short-term) memory that is mainly used by user session for local needs.
# Any attempt to allocate memory beyond this limit will be denied, and error will be reported to
# the user. Pay attention that the sum of max_mot_global_memory and max_mot_local_memory must not
# exceed max_process_memory configured postgresql.conf.
#
#max_mot_global_memory = 80%

# Configures the minimum memory limit for the global memory of the MOT engine.
# Specifying percentage value relates to the total defined by max_process_memory configured in
# postgresql.conf.
# This value is used both for pre-allocation of memory during startup, as well as for ensuring a
# minimum amount of memory is available for the MOT engine during its normal operation. When using
# compact storage policy (see store_memory_policy above), this value designates the lower limit
# under which memory is not released back to the kernel, but rather kept in the MOT engine for
# later reuse.
#
#min_mot_global_memory = 0

# Configures the maximum memory limit for the local memory of the MOT engine.
# Specifying percentage value relates to the total defined by max_process_memory configured in
# postgresql.conf.
# MOT engine memory is divided into global (long-term) memory that is mainly used to store user
# data, and local (short-term) memory that is mainly used by user session for local needs.
# Any attempt to allocate memory beyond this limit will be denied, and error will be reported to
# the user. Pay attention that the sum of max_mot_global_memory and max_mot_local_memory must not
# exceed max_process_memory configured postgresql.conf.
#
#max_mot_local_memory = 15%

# Configures the minimum memory limit for the local memory of the MOT engine.
# Specifying percentage value relates to the total defined by max_process_memory configured in
# postgresql.conf.
# This value is used both for pre-allocation of memory during startup, as well as for ensuring a
# minimum amount of memory is available for the MOT engine during its normal operation. When using
# compact storage policy (see store_memory_policy above), this value designates the lower limit
# under which memory is not released back to the kernel, but rather kept in the MOT engine for
# later reuse.
#
#min_mot_local_memory = 0

# Configures the maximum memory limit for a single session in MOT engine.
# Usually sessions in MOT engine are free to allocate local memory as much as needed, as long as
# the local memory limit is not breached. Nevertheless, in oder to prevent one session from taking
# too much memory, and thus denying memory from other sessions, this configuration item is used to
# restrict small session-local memory allocations (up to 1022 KB).
# Pay attention that this configuration item does not affect large and huge session-local memory
# allocations.
# A value of zero denotes no restriction on any session-local small allocations per session, except
# for the restriction arising from the local memory allocation limit configured by
# max_mot_local_memory.
# Note: Percentage values cannot be set for this configuration item.
#
#max_mot_session_memory = 0

# Configures the minimum memory reservation for a single session in MOT engine.
# This value is used both for pre-allocation of memory during session creation, as well as for
# ensuring a minimum amount of memory is available for the session to perform its normal operation.
# Note: Percentage values cannot be set for this configuration item.
#
#min_mot_session_memory = 0

# Configures the large buffer store for sessions.
# When a user session executes a query that requires a lot of memory (e.g. when using many rows),
# the large buffer store is used both for increasing the certainty level that such memory is
# available, and also for the purpose of serving this memory request more quickly. Any memory
# allocation for a session that is above 1022 KB is considered as a large memory allocation. If the
# large buffer store is not used or depleted, such allocations are treated as huge allocations that
# are served directly from kernel.
# Note: Percentage values cannot be set for this configuration item.
#
#session_large_buffer_store_size = 0

# Configures the maximum object size in the large allocation buffer store for sessions.
# Internally, the large buffer store is divided into objects of various sizes. This value is used
# for both setting an upper limit on objects originating from the large buffer store as well as for
# determining the internal division of the buffer store into objects of various size.
# This size cannot exceed 1/8 of the session_large_buffer_store_size, and if it does it is
# rectified to the possible maximum.
# Note: Percentage values cannot be set for this configuration item.
#
#session_large_buffer_store_max_object_size = 0

# Configures the maximum size of a single huge memory allocation made by a session.
# Huge allocations are served directly from kernel, and therefore are not guaranteed to succeed.
# This value also pertains to global (i.e. not session-related) memory allocations.
# Note: Percentage values cannot be set for this configuration item.
#
#session_max_huge_object_size = 1 GB

#------------------------------------------------------------------------------
# GARBAGE COLLECTION
#------------------------------------------------------------------------------

# Configures the memory threshold for the garbage collector.
# Each session manages its own list of to-be-reclaimed objects, and performs its own garbage
# collection during transaction commit. This value determines the total memory sum threshold of
# objects waiting to be reclaimed, above which garbage collection is triggered for a session.
# Generally speaking, the trade-off here is between un-reclaimed objects to garbage collection
# frequency. Setting a low value will keep low levels of un-reclaimed memory, but cause frequent
# garbage collection that might affect performance. Setting a high value will trigger garbage
# collection less frequently, but will result in higher levels of un-reclaimed memory. This is very
# much dependent upon overall workload.
# Note: Percentage values cannot be set for this configuration item.
#
#reclaim_threshold = 512 KB

# Configures the batch size for the garbage collection.
# The garbage collector reclaims memory for objects in batches. The purpose of this is to restrict
# the amount of object being reclaimed in one garbage collection pass, thus restricting the
# operation time of a single garbage collection pass.
#
#reclaim_batch_size = 8000

# Configures the high memory threshold for the garbage collection.
# Since the garbage collection is restricted to work in batches, it is possible that a session
# might have many objects that can be reclaimed, but were not. In such situations, in order to
# prevent too bloated garbage collection lists, this value is used to keep on reclaiming objects
# in the same pass, even though that batch size limit is reached, until the total size of still
# waiting to-be-reclaimed objects will be less than this threshold, or there are no more objects
# eligible for reclamation.
# Note: Percentage values cannot be set for this configuration item.
#
#high_reclaim_threshold = 8 MB

#------------------------------------------------------------------------------
# JIT
#------------------------------------------------------------------------------

# Specifies whether to use JIT compilation and execution.
# JIT execution allows preparing Just-In-Time compiled code for prepared queries during their
# planning phase and for stored procedures during their compilation phase. The resulting JIT-
# compiled function is executed whenever the prepared query or the stored procedure is invoked. JIT
# compilation takes place in the form of LLVM.
#
#enable_mot_codegen = false

# Specifies whether to use JIT query compilation and execution for planned queries.
# JIT query execution allows preparing Just-In-Time compiled code for a prepared query during its
# planning phase. The resulting JIT-compiled function is executed whenever the prepared query is
# invoked. JIT compilation takes place in the form of LLVM.
#
#enable_mot_query_codegen = true

# Specifies whether to use JIT query compilation and execution for stored procedures.
# JIT query execution allows preparing Just-In-Time compiled code for a stored procedure during its
# compilation phase. The resulting JIT-compiled function is executed whenever the stored procedure
# is invoked.
#
#enable_mot_sp_codegen = true

# Specifies whether to print emitted LLVM IR code for JIT-compiled queries.
#
#enable_mot_codegen_print = false

# Limits the amount of JIT queries allowed per user session.
#
#mot_codegen_limit = 50000

# Specifies whether to use JIT profiling.
# When using this option, the mot_jit_profile() function can be used to obtain run-time profile
# data for jitted stored procedures and queries.
#
#enable_mot_codegen_profile = true
